---
title:  "Navigating Computational Fluid Dynamics: A Tripartite Study in Parallelization and Domain Expansion"
mathjax: true
layout: post
categories: media
---

### Keywords
- C++
- CFD
- MPI
- Domain decomposition
- Parallel programming
- Arbitrary geometries
- Incompressible flow
- Subsonic flow
- Validation

Source Code: [Github](https://github.com/Erfan-Mashayekh/cfd-lab-project)

In the realm of scientific computing, we focused on exploring, enhancing, and optimizing computational fluid dynamics simulations. All these happened in a series of projects that happened in the CFD-Lab at the Technical University of Munich. Spanning three distinct projects, these modules offered a nuanced progression in understanding, hands-on implementation, and performance evaluation.

## Project 1: Foundations of Fluid Dynamics
Objective: Establish a foundational understanding of fluid dynamics simulations.
Tools Used: Sequential programming, numerical methods.
Outcomes: Grasp the essence of fluid dynamics simulations, understanding the Navier-Stokes equations, and implementing algorithms for flow visualization.
Key Implementation Steps: Primarily focused on single-threaded programming, concentrating on setting up simulations and visualizing flows.
Problems Addressed: Implementation of the Navier-Stokes solver, setting diverse boundary conditions, and showcasing results via visualization techniques.

## Project 2: Domain Expansion and Heat Transport
Objective: Extend the Navier-Stokes solver to accommodate arbitrary domain geometries and heat transport. Introduce the Boussinesq approximation.
Tools Used: PGM file format for domain shapes, and energy equation incorporation.
Outcomes: Understanding various boundary conditions for different geometries, learning to introduce domain shapes using PGM format, and integrating an energy equation into the simulation.
Key Implementation Steps: Extending the solver to incorporate arbitrary domain geometries, implementing new boundary conditions, and applying heat transport mechanisms.
Problems Addressed: Issues related to domain geometries, boundary conditions, and the introduction of heat transfer models into the simulations.

## Project 3: Parallelization with MPI
Objective: Parallelize fluid dynamics simulations using MPI, focusing on domain decomposition. Evaluate performance through scaling analyses.
Tools Used: MPI for parallel computing, domain decomposition.
Outcomes: Gain the ability to parallelize simulations using synchronous MPI communication and evaluate performance through strong and weak scaling tests.
Key Implementation Steps: Domain decomposition, communication algorithm adjustments, considerations of ghost layers, and detailed scaling analyses.


## Validation and Performance Scaling


The image displays how pressure spreads within a confined fluid. In this test, the flow is enclosed by warm and cold walls, causing a temperature decrease from left to right. To test the parallelization, the pressure field results from both parallel and sequential codes are compared. On the left side are the results from the code divided into six subdomains, three horizontally and two vertically. The right side displays the sequential code results. According to the image, the parallelization appears to have been successful. 

![Supersonic Contours](/images/cfdlab_fluidtrap.png)
*Pressure contour in the fluid trap test case. (left) parallelized solution ##(3 \times 2) $$ decomposed domain; (right) sequential solution with only $$ 1 $$ domain.*

### Strong Scaling

For the strong scaling analysis, Lid-Driven Cavity was employed to test the parallelization. In this scenario, the SOR solver's convergence rate was insufficient, leading to extended durations in obtaining results across various sizes, notably for 100x100 and finer grids. Due to time constraints, not all the results were attainable for comparison.



As the number of threads increased, it took longer to solve the Poisson equation fo pressure equation until convergence. When additional threads were added, a warning indicated that the SOR iterations exceeded the limit. This issue might stem from errors during the conversion of the algorithm to function in parallel, requiring further investigation.




